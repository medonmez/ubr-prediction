# ğŸ”´ Churn Prediction Model (Fine-Tuning)

Bu dokÃ¼man, mÃ¼ÅŸterinin churn (kayÄ±p) olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eden `churn_prediction_finetune.py` scriptinin teknik detaylarÄ±nÄ± aÃ§Ä±klar. Bu model, **Transfer Learning** yÃ¶ntemiyle, Ã¶nceden eÄŸitilmiÅŸ **Monad-EMDE Foundation Model** Ã¼zerine inÅŸa edilir.

---

## ğŸ“‹ Ä°Ã§indekiler

1. [AmaÃ§](#-amaÃ§)
2. [HÄ±zlÄ± BaÅŸlangÄ±Ã§](#-hÄ±zlÄ±-baÅŸlangÄ±Ã§)
3. [KonfigÃ¼rasyon Parametreleri](#ï¸-konfigÃ¼rasyon-parametreleri)
4. [Veri AkÄ±ÅŸÄ±](#-veri-akÄ±ÅŸÄ±)
5. [Model Mimarisi](#-model-mimarisi)
6. [EÄŸitim Stratejisi](#-eÄŸitim-stratejisi)
7. [DeÄŸerlendirme Metrikleri](#-deÄŸerlendirme-metrikleri)
8. [GÃ¶rselleÅŸtirmeler](#-gÃ¶rselleÅŸtirmeler)
9. [Ã‡Ä±ktÄ±lar](#-Ã§Ä±ktÄ±lar)
10. [Teknik Detaylar](#-teknik-detaylar)

---

## ğŸ¯ AmaÃ§

SÄ±fÄ±rdan bir model eÄŸitmek yerine, mÃ¼ÅŸterinin **gelecekteki davranÄ±ÅŸÄ±nÄ± (Future UBR)** Ã¶ÄŸrenmiÅŸ olan Foundation Model'in bilgisini kullanarak, daha az etiketli veri ile daha yÃ¼ksek performanslÄ± bir churn tahmini yapmaktÄ±r.

**Neden Transfer Learning?**
- Foundation Model, mÃ¼ÅŸterilerin davranÄ±ÅŸ Ã¶rÃ¼ntÃ¼lerini zaten Ã¶ÄŸrenmiÅŸtir
- Daha az etiketli veri ile daha iyi sonuÃ§lar elde edilir
- EÄŸitim sÃ¼resi Ã¶nemli Ã¶lÃ§Ã¼de kÄ±salÄ±r
- Modelin genelleme yeteneÄŸi artar

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# Sanal ortamÄ± aktif et
source venv/bin/activate

# Ã‡alÄ±ÅŸma dizinine git
cd scripts

# Fine-tuning iÅŸlemini baÅŸlat
python churn_prediction_finetune.py
```

> [!IMPORTANT]
> **Ã–n KoÅŸul:** Bu script Ã§alÄ±ÅŸtÄ±rÄ±lmadan Ã¶nce aÅŸaÄŸÄ±daki adÄ±mlar tamamlanmÄ±ÅŸ olmalÄ±dÄ±r:
> 1. `bank_emde_session.py` â†’ EMDE sketch'lerinin Ã¼retilmesi
> 2. `future_ubr_ffn.py` â†’ Foundation Model'in eÄŸitilmesi

Ä°ÅŸlem GPU/MPS Ã¼zerinde ~5 dakikadan kÄ±sa sÃ¼rer.

---

## âš™ï¸ KonfigÃ¼rasyon Parametreleri

Script baÅŸÄ±nda tanÄ±mlanan parametreler:

### Dizin AyarlarÄ±
| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `DATA_DIR` | `../data/emde` | EMDE sketch dosyalarÄ±nÄ±n konumu |
| `FOUNDATION_MODEL_DIR` | `../data/ffn_model` | Pre-trained Foundation Model konumu |
| `OUTPUT_DIR` | `../data/churn_model` | Ã‡Ä±ktÄ± dosyalarÄ±nÄ±n kaydedileceÄŸi yer |
| `WALK` | `4` | KullanÄ±lacak Cleora walk sayÄ±sÄ± (DLSH ile) |

### EÄŸitim Hiperparametreleri
| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `FREEZE_BACKBONE` | `False` | Backbone aÄŸÄ±rlÄ±klarÄ±nÄ± dondur/aÃ§ |
| `LEARNING_RATE` | `1e-4` | Temel Ã¶ÄŸrenme hÄ±zÄ± |
| `BATCH_SIZE` | `256` | Mini-batch boyutu |
| `EPOCHS` | `50` | Maksimum epoch sayÄ±sÄ± |
| `PATIENCE` | `10` | Early stopping sabÄ±r deÄŸeri |
| `LEAKY_RELU_SLOPE` | `0.01` | LeakyReLU negatif eÄŸim |
| `SEED` | `42` | Rastgelelik tohumu (reproducibility) |

### Cihaz SeÃ§imi
Script otomatik olarak en uygun cihazÄ± seÃ§er:
```python
DEVICE = "cuda" if torch.cuda.is_available() 
         else "mps" if torch.backends.mps.is_available() 
         else "cpu"
```

---

## ğŸ“Š Veri AkÄ±ÅŸÄ±

### 1. Veri YÃ¼kleme (`load_data` fonksiyonu)

```
emde_session_sketches_walk4.npz
â”œâ”€â”€ past_sketches      â†’ [N, 320] - GeÃ§miÅŸ davranÄ±ÅŸ sketch'i (10 subspace Ã— 32 bin)
â”œâ”€â”€ portfolio_sketches â†’ [N, 320] - PortfÃ¶y sketch'i (10 subspace Ã— 32 bin)
â””â”€â”€ churn_labels       â†’ [N] - Binary churn etiketi (0/1)
```

### 2. ChurnDataset SÄ±nÄ±fÄ±

```python
class ChurnDataset(Dataset):
    def __init__(self, past_sketches, portfolio_sketches, churn_labels):
        # Past ve Portfolio sketch'lerini birleÅŸtir
        self.X = np.concatenate([past_sketches, portfolio_sketches], axis=1)
        # X boyutu: [N, 640] (320 + 320)
        self.y = churn_labels
```

**Girdi VektÃ¶rÃ¼ YapÄ±sÄ± (640 boyut):**
```
[0:320]      â†’ Past UBR Sketch (dinamik davranÄ±ÅŸ)
[320:640]    â†’ Portfolio Sketch (statik Ã¼rÃ¼n sahipliÄŸi)
```

### 3. Veri BÃ¶lme

```
+------------------+
|   Toplam Veri    | 100% (N mÃ¼ÅŸteri)
+--------+---------+
         |
    +----+----+----+
    |    |    |    |
  Train Val  Test
   70%  15%  15%
```

---

## ğŸ§  Model Mimarisi

### Genel YapÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ChurnPredictor Model                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Input [640]                                                â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚ L2 Normalizeâ”‚  F.normalize(x, p=2, dim=-1)              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼                                                     â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘           BACKBONE (From Foundation Model)           â•‘   â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£   â”‚
â”‚   â•‘  Input Projection                                    â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ Linear(640 â†’ 3000)                             â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ BatchNorm1d(3000)                              â•‘   â”‚
â”‚   â•‘  â””â”€â”€ LeakyReLU(0.01)                                â•‘   â”‚
â”‚   â•‘                                                      â•‘   â”‚
â”‚   â•‘  Residual Blocks (x3)                               â•‘   â”‚
â”‚   â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘   â”‚
â”‚   â•‘  â”‚ Linear(3000 â†’ 3000)               â”‚             â•‘   â”‚
â”‚   â•‘  â”‚ BatchNorm1d(3000)                 â”‚â”€â”€â”          â•‘   â”‚
â”‚   â•‘  â”‚ LeakyReLU(0.01)                   â”‚  â”‚ Skip     â•‘   â”‚
â”‚   â•‘  â”‚ Dropout(0.1)                      â”‚  â”‚ Connectionâ•‘   â”‚
â”‚   â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â•‘   â”‚
â”‚   â•‘            +  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â•‘   â”‚
â”‚   â•‘                                                      â•‘   â”‚
â”‚   â•‘  Output Projection                                   â•‘   â”‚
â”‚   â•‘  â””â”€â”€ Linear(3000 â†’ 320)                             â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼ [320]                                               â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
â”‚   â•‘           CLASSIFICATION HEAD (New)                  â•‘   â”‚
â”‚   â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£   â”‚
â”‚   â•‘  â”œâ”€â”€ Linear(320 â†’ 256)                              â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ BatchNorm1d(256)                               â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ LeakyReLU(0.01)                                â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ Dropout(0.3)                                   â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ Linear(256 â†’ 64)                               â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ BatchNorm1d(64)                                â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ LeakyReLU(0.01)                                â•‘   â”‚
â”‚   â•‘  â”œâ”€â”€ Dropout(0.2)                                   â•‘   â”‚
â”‚   â•‘  â””â”€â”€ Linear(64 â†’ 1)                                 â•‘   â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚        â”‚                                                     â”‚
â”‚        â–¼                                                     â”‚
â”‚   Output: Logits [1]                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. Backbone (Omurga) - Foundation Model'den Transfer

Foundation Model'in (`future_ubr_model_walk4.pt`) eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klarÄ± yÃ¼klenir:

**YÃ¼kleme SÃ¼reci:**
```python
checkpoint = torch.load(foundation_model_path, ...)
input_dim = checkpoint['input_dim']      # 640 (320 + 320)
hidden_dim = checkpoint['hidden_dim']    # 3000 (~3000 nÃ¶ron, paper spec)
num_layers = checkpoint['num_layers']    # 3
output_dim = checkpoint['output_dim']    # 320
```

**AÄŸÄ±rlÄ±k AktarÄ±mÄ±:**
- `input_proj` â†’ Linear + BN + LeakyReLU
- `residual_blocks` â†’ 3 adet ResidualBlock
- `output_proj` â†’ Linear (LogSoftmax olmadan!)

> [!NOTE]
> Orijinal Foundation Model'in sonundaki `LogSoftmax` katmanÄ± **atÄ±lÄ±r**, Ã§Ã¼nkÃ¼ artÄ±k density estimation deÄŸil, binary sÄ±nÄ±flandÄ±rma yapÄ±yoruz.

### 2. ResidualBlock (Monad-EMDE Paper'dan)

```python
class ResidualBlock(nn.Module):
    def __init__(self, dim=3000, dropout=0.1):
        self.block = nn.Sequential(
            nn.Linear(dim, dim),
            nn.BatchNorm1d(dim),
            nn.LeakyReLU(negative_slope=0.01),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        return self.block(x) + x  # â† Skip connection
```

### 3. Classification Head (Yeni Eklenen)

Backbone'un Ã§Ä±kardÄ±ÄŸÄ± 320 boyutlu Ã¶znitelik vektÃ¶rÃ¼nÃ¼ alÄ±p churn olasÄ±lÄ±ÄŸÄ±na (0-1 arasÄ±) Ã§evirir:

| Katman | Girdi â†’ Ã‡Ä±ktÄ± | AÃ§Ä±klama |
|--------|---------------|----------|
| Linear + BN + LeakyReLU | 320 â†’ 256 | Feature reduction |
| Dropout(0.3) | 256 â†’ 256 | Overfitting engelleyici |
| Linear + BN + LeakyReLU | 256 â†’ 64 | Daha fazla sÄ±kÄ±ÅŸtÄ±rma |
| Dropout(0.2) | 64 â†’ 64 | Overfitting engelleyici |
| Linear | 64 â†’ 1 | Binary Ã§Ä±ktÄ± (logit) |

---

## ğŸ“š EÄŸitim Stratejisi

### 1. Loss Function: Weighted BCEWithLogitsLoss

Churn verisi dengesizdir (Ã–rn: %20 Churn, %80 Retained). Standart loss fonksiyonu, Ã§oÄŸunluk sÄ±nÄ±fÄ±nÄ± (Retained) tahmin etmeye odaklanÄ±r.

**Ã‡Ã¶zÃ¼m: Pozitif SÄ±nÄ±f AÄŸÄ±rlÄ±klandÄ±rmasÄ±**

```python
n_positive = churn_labels.sum()      # Churn eden mÃ¼ÅŸteri sayÄ±sÄ±
n_negative = len(labels) - n_positive # Kalan mÃ¼ÅŸteri sayÄ±sÄ±
pos_weight = n_negative / n_positive  # ~4.0 (veri setine gÃ¶re deÄŸiÅŸir)

criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
```

**Etki:** Model, bir churn mÃ¼ÅŸterisini kaÃ§Ä±rdÄ±ÄŸÄ±nda ~4 kat daha fazla ceza alÄ±r.

### 2. Optimizer: AdamW with Differential Learning Rates

```python
# Backbone: DÃ¼ÅŸÃ¼k LR (mevcut bilgiyi koru)
backbone_params = [...input_proj, residual_blocks, output_proj...]
backbone_lr = 1e-4

# Classifier: YÃ¼ksek LR (sÄ±fÄ±rdan Ã¶ÄŸren)
classifier_lr = 1e-3  # (10x backbone)

optimizer = optim.AdamW([
    {'params': backbone_params, 'lr': backbone_lr},
    {'params': model.classifier.parameters(), 'lr': classifier_lr}
])
```

**Neden FarklÄ± Learning Rate?**
- **Backbone:** Zaten deÄŸerli bilgi iÃ§eriyor â†’ yavaÅŸ gÃ¼ncelle, bilgiyi koru
- **Classifier:** SÄ±fÄ±rdan baÅŸlÄ±yor â†’ hÄ±zlÄ± Ã¶ÄŸren, adapte ol

### 3. Learning Rate Scheduler

```python
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min',      # Val loss azaldÄ±ÄŸÄ±nda
    factor=0.5,      # LR'yi yarÄ±ya indir
    patience=5       # 5 epoch iyileÅŸme olmazsa
)
```

### 4. Early Stopping

```python
PATIENCE = 10  # 10 epoch boyunca val_loss iyileÅŸmezse dur

if val_loss < best_val_loss:
    best_val_loss = val_loss
    best_model_state = model.state_dict().copy()
    patience_counter = 0
else:
    patience_counter += 1
    
if patience_counter >= PATIENCE:
    model.load_state_dict(best_model_state)  # En iyi modele geri dÃ¶n
    break
```

### 5. Gradient Clipping

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

Gradientlerin patlamasÄ±nÄ± Ã¶nler, eÄŸitimi stabilize eder.

### 6. Backbone Freezing (Opsiyonel)

```python
FREEZE_BACKBONE = False  # True yapÄ±lÄ±rsa:

# Backbone parametreleri dondurulur
for param in self.input_proj.parameters():
    param.requires_grad = False
for block in self.residual_blocks:
    for param in block.parameters():
        param.requires_grad = False
for param in self.output_proj.parameters():
    param.requires_grad = False
```

| Mod | Trainable Params | KullanÄ±m Durumu |
|-----|------------------|-----------------|
| `FREEZE_BACKBONE=True` | ~850K (sadece classifier) | Az veri, hÄ±zlÄ± eÄŸitim |
| `FREEZE_BACKBONE=False` | ~5M+ (tÃ¼m model) | Ã‡ok veri, daha iyi performans |

---

## ğŸ“ˆ DeÄŸerlendirme Metrikleri

### Test Seti Metrikleri

| Metrik | AnlamÄ± | Hedef |
|--------|--------|-------|
| **ROC-AUC** | Modelin churn ve retained sÄ±nÄ±flarÄ±nÄ± ayÄ±rma yeteneÄŸi | > 0.85 |
| **Recall** | GerÃ§ekten churn edenlerin kaÃ§Ä±nÄ± yakaladÄ±k? (**En kritik**) | > 0.70 |
| **Precision** | "Churn edecek" dediklerimizin kaÃ§Ä± gerÃ§ekten etti? | > 0.60 |
| **F1 Score** | Precision ve Recall'un harmonik ortalamasÄ± | > 0.65 |
| **Accuracy** | Genel doÄŸruluk oranÄ± | > 0.80 |
| **Average Precision** | Precision-Recall eÄŸrisi altÄ±ndaki alan | > 0.70 |

### Confusion Matrix Yorumlama

```
                  Predicted
              Retained  Churned
Actual  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Retainedâ”‚     TN     â”‚     FP     â”‚  â† False alarm (gereksiz kampanya)
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Churned â”‚     FN     â”‚     TP     â”‚  â† KaÃ§Ä±rÄ±lan churn (kritik!)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **False Negative (FN):** En tehlikeli! Churn edecek mÃ¼ÅŸteriyi kaÃ§Ä±rdÄ±k
- **False Positive (FP):** Gereksiz retention kampanyasÄ± (maliyet var ama kabul edilebilir)

---

## ğŸ“Š GÃ¶rselleÅŸtirmeler

Script iki kapsamlÄ± gÃ¶rselleÅŸtirme dosyasÄ± Ã¼retir:

### 1. `churn_prediction_results.png` (2x2 Grid)

| Konum | Grafik | AÃ§Ä±klama |
|-------|--------|----------|
| Sol Ãœst | Training & Validation Loss | Epoch bazÄ±nda BCE loss deÄŸiÅŸimi |
| SaÄŸ Ãœst | Validation AUC | Epoch bazÄ±nda AUC deÄŸiÅŸimi + en iyi deÄŸer |
| Sol Alt | Prediction Distribution | Churn vs Retained tahmin olasÄ±lÄ±klarÄ± histogramÄ± |
| SaÄŸ Alt | ROC Curve | True Positive Rate vs False Positive Rate |

### 2. `churn_advanced_kpis.png` (2x3 Grid)

| Konum | Grafik | AÃ§Ä±klama |
|-------|--------|----------|
| Sol Ãœst | Precision-Recall Curve | Ä°mbalanced data iÃ§in Ã¶nemli |
| Orta Ãœst | Confusion Matrix Heatmap | SÄ±nÄ±flandÄ±rma doÄŸruluÄŸu detayÄ± |
| SaÄŸ Ãœst | Calibration Plot | Tahmin olasÄ±lÄ±klarÄ±nÄ±n gÃ¼venilirliÄŸi |
| Sol Alt | Metrics vs Threshold | FarklÄ± threshold'larda Precision/Recall/F1 |
| Orta Alt | Cumulative Gain (Lift) Chart | Modelin random'a gÃ¶re Ã¼stÃ¼nlÃ¼ÄŸÃ¼ |
| SaÄŸ Alt | Summary Metrics Table | TÃ¼m KPI'larÄ±n Ã¶zet tablosu |

**Lift Chart Yorumlama:**
```
Ã–rnek: "Lift @ 20% = 2.5x" 
â†’ En riskli %20 mÃ¼ÅŸteriyi hedeflediÄŸimizde, 
  rastgele seÃ§ime gÃ¶re 2.5 kat daha fazla churn yakalÄ±yoruz.
```

---

## ğŸ“ Ã‡Ä±ktÄ±lar

| Dosya | Konum | AÃ§Ä±klama |
|-------|-------|----------|
| `churn_predictor_walk4.pt` | `data/churn_model/` | EÄŸitilmiÅŸ model checkpoint'i |
| `churn_predictions_walk4.npz` | `data/churn_model/` | Test seti tahminleri ve etiketleri |
| `churn_prediction_results.png` | `data/churn_model/` | Temel eÄŸitim grafikleri |
| `churn_advanced_kpis.png` | `data/churn_model/` | GeliÅŸmiÅŸ KPI grafikleri |

### Model Checkpoint Ä°Ã§eriÄŸi

```python
torch.save({
    'model_state_dict': model.state_dict(),
    'freeze_backbone': FREEZE_BACKBONE,
    'walk': WALK,
    'metrics': {
        'accuracy': float,
        'precision': float,
        'recall': float,
        'f1': float,
        'auc': float
    },
    'architecture': 'MonadEMDE_ChurnPredictor'
}, model_path)
```

### Predictions NPZ Ä°Ã§eriÄŸi

```python
np.savez_compressed(pred_path, 
    predictions=preds,  # [N_test] - Churn olasÄ±lÄ±klarÄ± (0-1)
    labels=labels,      # [N_test] - GerÃ§ek etiketler (0/1)
    metrics=metrics     # Dict - Test metrikleri
)
```

---

## ğŸ”§ Teknik Detaylar

### Forward Pass AkÄ±ÅŸÄ±

```python
def forward(self, x):
    # 1. L2 Normalization (Foundation Model ile tutarlÄ±lÄ±k)
    x = F.normalize(x, p=2, dim=-1)  # [B, 640] â†’ [B, 640]
    
    # 2. Input Projection
    h = self.input_proj(x)  # [B, 640] â†’ [B, 3000]
    
    # 3. Residual Blocks (x3)
    for block in self.residual_blocks:
        h = block(h)  # [B, 3000] â†’ [B, 3000] (skip connection ile)
    
    # 4. Output Projection
    features = self.output_proj(h)  # [B, 3000] â†’ [B, 320]
    
    # 5. Classification Head
    logits = self.classifier(features)  # [B, 320] â†’ [B, 1]
    
    return logits.squeeze(-1)  # [B]
```

### Ä°nference (Tahmin Yapma)

```python
model.eval()
with torch.no_grad():
    logits = model(input_tensor)
    probabilities = torch.sigmoid(logits)  # Logits â†’ 0-1 olasÄ±lÄ±k
    predictions = (probabilities >= 0.5).int()  # Binary sÄ±nÄ±flandÄ±rma
```

### EÄŸitim DÃ¶ngÃ¼sÃ¼ Ã–zeti

```
For each epoch:
    1. model.train()
    2. For each batch:
        a. Forward pass â†’ logits
        b. Compute weighted BCE loss
        c. Backward pass
        d. Gradient clipping (max_norm=1.0)
        e. Optimizer step
    3. model.eval()
    4. Validation loop â†’ val_loss, val_auc
    5. Learning rate scheduler step
    6. Early stopping check
```

---

## ğŸ’¼ KullanÄ±m Senaryosu

Bu modelin Ã§Ä±ktÄ±sÄ± (Churn Skoru), pazarlama departmanÄ± tarafÄ±ndan Ã¶nleyici aksiyonlar almak iÃ§in kullanÄ±labilir:

```
Churn Score > 0.7  â†’ YÃ¼ksek Risk  â†’ Acil arama + Ã¶zel kampanya
Churn Score > 0.5  â†’ Orta Risk   â†’ Email kampanyasÄ±
Churn Score > 0.3  â†’ DÃ¼ÅŸÃ¼k Risk  â†’ Genel bilgilendirme
Churn Score < 0.3  â†’ GÃ¼venli     â†’ Standart hizmet
```

---

## ğŸ“š BaÄŸÄ±mlÄ±lÄ±klar

```python
import torch                  # PyTorch core
import torch.nn as nn         # Neural network modÃ¼lleri
import torch.nn.functional as F  # Fonksiyonel operasyonlar
import torch.optim as optim   # Optimizasyon algoritmalarÄ±
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, average_precision_score
)
from sklearn.calibration import calibration_curve
```
